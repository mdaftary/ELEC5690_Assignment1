{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "T1KpksCxZ7F6"
      },
      "source": [
        "#**Problem 1: Skin Lesion Image Classification**#\n",
        "The dataset is the ISIC2018 task 3, including 10,015 training images, 193 validation images, and\n",
        "1,512 test images with seven diseases."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uNbTMcvLbBDX"
      },
      "source": [
        "#### **Install Libraries**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AH7PCNyy2SR1"
      },
      "outputs": [],
      "source": [
        "!pip install torch\n",
        "!pip install torchvision\n",
        "!pip install pandas\n",
        "!pip install scikit-learn\n",
        "!pip install tqdm"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "u35YJLT8bgZc"
      },
      "source": [
        "#### **Check for GPU if using google colab**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_dkOBj4Xtu17"
      },
      "outputs": [],
      "source": [
        "!nvidia-smi"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JZYz1MaZbo1G"
      },
      "source": [
        "#### **Mount Drive if needed**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OAQaoODvrCrg"
      },
      "outputs": [],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_keyV9Vbcquc"
      },
      "source": [
        "#### **Import Libraries**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "sWnpTzKB2bwH"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import pandas as pd\n",
        "from PIL import Image\n",
        "import torch\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from torchvision import transforms\n",
        "\n",
        "import torch.nn as nn\n",
        "from torchvision import models\n",
        "import torch.optim as optim\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "from sklearn.metrics import classification_report"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qE_LbTORbunl"
      },
      "source": [
        "#### **Define the dataloader and load the data**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "2xFMQZJcH9hI"
      },
      "outputs": [],
      "source": [
        "class SkinLesionDataset(Dataset):\n",
        "    def __init__(self, images_dir, labels_file, transform=None):\n",
        "        self.images_dir = images_dir\n",
        "        self.labels_df = pd.read_csv(labels_file)\n",
        "        self.transform = transform\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.labels_df)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        # Get the image file name\n",
        "        image_name = self.labels_df.iloc[idx, 0]\n",
        "        image_path = os.path.join(self.images_dir, image_name)\n",
        "        image_path = f\"{image_path}.jpg\"\n",
        "\n",
        "\n",
        "        # Load the image\n",
        "        image = Image.open(image_path).convert(\"RGB\")\n",
        "\n",
        "        # Get the labels and convert to single class index\n",
        "        labels = self.labels_df.iloc[idx, 1:].values.astype(float)  # Select all label columns\n",
        "        class_idx = labels.argmax()  # Get the index of the max value\n",
        "\n",
        "        if self.transform:\n",
        "            image = self.transform(image)\n",
        "\n",
        "        return image, class_idx  # Return the image and the class index\n",
        "\n",
        "# Directory and file paths\n",
        "train_images_dir = '/content/drive/MyDrive/ISIC2018_Task3_Training_Input' # PATH to training images folder\n",
        "train_labels_file = os.path.join(train_images_dir, 'ISIC2018_Task3_Training_GroundTruth.csv') # training labels file\n",
        "val_images_dir = '/content/drive/MyDrive/ISIC2018_Task3_Validation_Input' # PATH to validation images folder\n",
        "val_labels_file = os.path.join(val_images_dir, 'ISIC2018_Task3_Validation_GroundTruth.csv')# validation labels file\n",
        "test_images_dir = '/content/drive/MyDrive/ISIC2018_Task3_Test_Input' # PATH to testing images folder\n",
        "test_labels_file = os.path.join(test_images_dir, 'ISIC2018_Task3_Test_GroundTruth.csv')# testing labels file\n",
        "\n",
        "# Define image transformations\n",
        "transform = transforms.Compose([\n",
        "    transforms.Resize((224, 224)),\n",
        "    transforms.ToTensor(),\n",
        "])\n",
        "\n",
        "# Create dataset and dataloader\n",
        "train_dataset = SkinLesionDataset(train_images_dir, train_labels_file, transform=transform)\n",
        "train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)\n",
        "val_dataset = SkinLesionDataset(val_images_dir, val_labels_file, transform=transform)\n",
        "val_loader = DataLoader(val_dataset, batch_size=25, shuffle=True)\n",
        "test_dataset = SkinLesionDataset(test_images_dir, test_labels_file, transform=transform)\n",
        "test_loader = DataLoader(test_dataset, batch_size=32, shuffle=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Q-jX4m5Bsno2"
      },
      "source": [
        "####***OPTIONAL modules (DO NOT RUN)***####\n",
        "function to calculate class weights. This is not used in our training configurations."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cL62asnYBNFm"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.utils import class_weight\n",
        "\n",
        "# Load the labels and count class instances\n",
        "def compute_class_weights(labels_file):\n",
        "    labels_df = pd.read_csv(labels_file)\n",
        "    all_labels = labels_df.values[:, 1:]  # Assuming labels are in the second column onward\n",
        "    class_indices = all_labels.argmax(axis=1)  # Get the index of the max value for each row\n",
        "    class_weights = class_weight.compute_class_weight('balanced', classes=np.unique(class_indices), y=class_indices)\n",
        "    return torch.tensor(class_weights, dtype=torch.float).to(device)\n",
        "\n",
        "\n",
        "# Compute class weights\n",
        "class_weights = compute_class_weights(train_labels_file)\n",
        "\n",
        "print(\"Class Weights:\", class_weights)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kKO2f51CtyoF"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "\n",
        "class ContrastiveLoss(nn.Module):\n",
        "    def __init__(self, margin=1.0):\n",
        "        super(ContrastiveLoss, self).__init__()\n",
        "        self.margin = margin\n",
        "\n",
        "    def forward(self, output1, output2, label):\n",
        "        # Calculate the Euclidean distance between the two outputs\n",
        "        euclidean_distance = nn.functional.pairwise_distance(output1, output2)\n",
        "\n",
        "        # Compute the contrastive loss\n",
        "        loss = (1 - label) * torch.pow(euclidean_distance, 2) + \\\n",
        "               (label) * torch.pow(torch.clamp(self.margin - euclidean_distance, min=0.0), 2)\n",
        "        return loss.mean()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0Gt36-W_5Qst"
      },
      "source": [
        "#### **CONFIGURATION 1**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "metadata": {
        "id": "OcVAoxPqswiY"
      },
      "outputs": [],
      "source": [
        "from tqdm import tqdm\n",
        "\n",
        "def train_model(model, train_loader, val_loader, criterion, optimizer, num_epochs=10):\n",
        "    best_val_accuracy = 0\n",
        "    for epoch in range(num_epochs):\n",
        "        model.train()\n",
        "        train_running_loss = 0.0\n",
        "        correct = 0\n",
        "        total = 0\n",
        "\n",
        "        # setup training progress bar\n",
        "        train_loop = tqdm(enumerate(train_loader), total=len(train_loader), leave= False)\n",
        "\n",
        "        for batch_idx, (images, labels) in train_loop:\n",
        "            images, labels = images.to(device), labels.to(device)\n",
        "            optimizer.zero_grad()\n",
        "            outputs = model(images)\n",
        "\n",
        "            # recording loss\n",
        "            loss = criterion(outputs, labels)\n",
        "            batch_train_loss.append(loss.item())\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "            train_running_loss += loss.item()\n",
        "\n",
        "            # recording accuracy\n",
        "            _, predicted = torch.max(outputs.data, 1)\n",
        "            total += labels.size(0)\n",
        "            batch_correct = (predicted == labels).sum().item()\n",
        "            correct += batch_correct\n",
        "            acc = 100 * batch_correct/labels.size(0)\n",
        "            batch_train_acc.append(acc)\n",
        "\n",
        "            # update progress bar\n",
        "            train_loop.set_description(f\"Epoch [{epoch+1}/{num_epochs}]\")\n",
        "            train_loop.set_postfix(loss=loss.item(), acc=acc)\n",
        "\n",
        "\n",
        "\n",
        "        # Calculate training accuracy\n",
        "        epoch_train_accuracy = 100 * correct / total\n",
        "\n",
        "        # Validation\n",
        "        model.eval()\n",
        "        val_correct = 0\n",
        "        val_total = 0\n",
        "        val_running_loss = 0.0\n",
        "\n",
        "        # setup validation progress bar\n",
        "        val_loop = tqdm(enumerate(val_loader), total=len(val_loader), leave= False)\n",
        "\n",
        "        with torch.no_grad():\n",
        "            for batch_idx, (images, labels) in val_loop:\n",
        "                images, labels = images.to(device), labels.to(device)\n",
        "                outputs = model(images)\n",
        "\n",
        "                 # recording loss\n",
        "                loss = criterion(outputs, labels)\n",
        "                batch_val_loss.append(loss.item())\n",
        "                val_running_loss += loss.item()\n",
        "\n",
        "                # recording accuracy\n",
        "                _, predicted = torch.max(outputs.data, 1)\n",
        "                val_total += labels.size(0)\n",
        "                batch_correct = (predicted == labels).sum().item()\n",
        "                val_correct += batch_correct\n",
        "                acc = 100 * batch_correct/labels.size(0)\n",
        "                batch_val_acc.append(acc)\n",
        "\n",
        "                # update progress bar\n",
        "                val_loop.set_description(f\"Epoch [{epoch+1}/{num_epochs}]\")\n",
        "                val_loop.set_postfix(loss=loss, acc=acc)\n",
        "\n",
        "        # Calculate validation accuracy\n",
        "        epoch_val_accuracy = 100 * val_correct / val_total\n",
        "\n",
        "        # Print progress\n",
        "        print(f'Epoch [{epoch+1}/{num_epochs}], '\n",
        "              f'Train Loss: {train_running_loss/len(train_loader):.4f}, '\n",
        "              f'Train Accuracy: {epoch_train_accuracy:.2f}%, '\n",
        "              f'Val Loss: {val_running_loss/len(val_loader):.4f}, '\n",
        "              f'Val Accuracy: {epoch_val_accuracy:.2f}%')\n",
        "\n",
        "        # Save the model if the validation accuracy is the best\n",
        "        if epoch_val_accuracy > best_val_accuracy:\n",
        "            best_val_accuracy = epoch_val_accuracy\n",
        "            torch.save(model.state_dict(), 'best_model.pth')\n",
        "            print(f'Saved model with accuracy: {best_val_accuracy:.2f}%')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "metadata": {
        "id": "VFppA1vgtg2e"
      },
      "outputs": [],
      "source": [
        "# CONFIGURATION 1\n",
        "\n",
        "# Initialise loss and accuracy lists\n",
        "batch_train_loss = [] # Training loss\n",
        "batch_val_loss = [] # Validation loss\n",
        "batch_train_acc = [] # Training accuracy\n",
        "batch_val_acc = [] # Validation accuracy\n",
        "\n",
        "import torchvision\n",
        "\n",
        "# Load pre-trained ResNet50\n",
        "model = models.resnet50(weights=torchvision.models.ResNet50_Weights.DEFAULT,progress=True)\n",
        "\n",
        "# Modify the final layer for multi-label classification\n",
        "num_classes = 7  # Number of skin diseases\n",
        "model.fc = nn.Linear(model.fc.in_features, num_classes)\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "model = model.to(device)\n",
        "\n",
        "# Define loss function and optimizer\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = optim.Adam(model.parameters(), lr=0.001)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fnVSehFqtDGJ"
      },
      "outputs": [],
      "source": [
        "train_model(model, train_loader, val_loader, criterion, optimizer, num_epochs=15)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AuD5AEg15ZT-"
      },
      "source": [
        "#### **CONFIGURATION 2 Note: If you run this, the previous configuration data will be replaced**\n",
        "\n",
        "Run the evaluation metrics in the next section first before running configuration 2 if you want to view results for configuration 1"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "4fS_lClZ8Cfx"
      },
      "outputs": [],
      "source": [
        "from tqdm import tqdm\n",
        "import torch\n",
        "import torch.nn.functional as F\n",
        "\n",
        "class FocalLoss(nn.Module):\n",
        "    def __init__(self, alpha=1.0, gamma=2.0, reduction='mean'):\n",
        "        super(FocalLoss, self).__init__()\n",
        "        self.alpha = alpha\n",
        "        self.gamma = gamma\n",
        "        self.reduction = reduction\n",
        "\n",
        "    def forward(self, inputs, targets):\n",
        "        BCE_loss = F.cross_entropy(inputs, targets, reduction='none')\n",
        "        pt = torch.exp(-BCE_loss)  # Probability of true class\n",
        "        F_loss = self.alpha * (1 - pt) ** self.gamma * BCE_loss\n",
        "\n",
        "        if self.reduction == 'mean':\n",
        "            return F_loss.mean()\n",
        "        elif self.reduction == 'sum':\n",
        "            return F_loss.sum()\n",
        "        else:\n",
        "            return F_loss\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "PUe2FXGIf9p2"
      },
      "outputs": [],
      "source": [
        "def train_focal_model(model, train_loader, val_loader, criterion, optimizer, num_epochs=10):\n",
        "    best_val_accuracy = 0\n",
        "    for epoch in range(num_epochs):\n",
        "        model.train()\n",
        "        train_running_loss = 0.0\n",
        "        train_correct = 0\n",
        "        train_total = 0\n",
        "\n",
        "        # Training loop\n",
        "        train_loop = tqdm(enumerate(train_loader), total=len(train_loader), leave=False)\n",
        "\n",
        "        for batch_idx, (images, labels) in train_loop:\n",
        "            images, labels = images.to(device), labels.to(device)\n",
        "            optimizer.zero_grad()\n",
        "\n",
        "            # Forward pass\n",
        "            outputs = model(images)\n",
        "\n",
        "            # Calculate Focal Loss\n",
        "            loss = criterion(outputs, labels)\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "\n",
        "            # Update running loss\n",
        "            train_running_loss += loss.item()\n",
        "\n",
        "            # Calculate accuracy\n",
        "            _, preds = torch.max(outputs, 1)  # Get predicted class\n",
        "            correct = (preds == labels).sum().item()\n",
        "            train_correct += correct\n",
        "            total = labels.size(0)\n",
        "            train_total += total\n",
        "            acc = (correct / total) * 100\n",
        "\n",
        "            batch_train_loss.append(loss.item())\n",
        "            batch_train_acc.append(acc)\n",
        "\n",
        "            # Update progress bar\n",
        "            train_loop.set_description(f\"Epoch [{epoch + 1}/{num_epochs}]\")\n",
        "            train_loop.set_postfix(loss=loss.item(), acc=acc)\n",
        "\n",
        "        # Calculate training accuracy\n",
        "        epoch_train_accuracy = 100 * train_correct / train_total\n",
        "        avg_train_loss = train_running_loss / len(train_loader)\n",
        "\n",
        "        # Validation\n",
        "        model.eval()\n",
        "        val_correct = 0\n",
        "        val_total = 0\n",
        "        val_running_loss = 0.0\n",
        "\n",
        "        val_loop = tqdm(enumerate(val_loader), total=len(val_loader), leave=False)\n",
        "\n",
        "        with torch.no_grad():\n",
        "            for batch_idx, (images, labels) in val_loop:\n",
        "                images, labels = images.to(device), labels.to(device)\n",
        "\n",
        "                outputs = model(images)\n",
        "\n",
        "                # Calculate Focal Loss\n",
        "                loss = criterion(outputs, labels)\n",
        "                val_running_loss += loss.item()\n",
        "\n",
        "                # Calculate accuracy\n",
        "                _, preds = torch.max(outputs, 1)\n",
        "                correct = (preds == labels).sum().item()\n",
        "                val_correct += correct\n",
        "                val_total += labels.size(0)\n",
        "                acc = (correct / labels.size(0)) * 100\n",
        "\n",
        "                batch_val_loss.append(loss.item())\n",
        "                batch_val_acc.append(acc)\n",
        "\n",
        "                # Update progress bar\n",
        "                val_loop.set_description(f\"Epoch [{epoch + 1}/{num_epochs}]\")\n",
        "                val_loop.set_postfix(loss=loss.item(), acc=acc)\n",
        "\n",
        "        # Calculate validation accuracy\n",
        "        epoch_val_accuracy = 100 * val_correct / val_total\n",
        "        avg_val_loss = val_running_loss / len(val_loader)\n",
        "\n",
        "        # Save the model if the validation accuracy is the best\n",
        "        if epoch_val_accuracy > best_val_accuracy:\n",
        "            best_val_accuracy = epoch_val_accuracy\n",
        "            torch.save(model.state_dict(), 'best_model.pth')\n",
        "            print(f'Saved model with validation accuracy: {best_val_accuracy:.2f}%')\n",
        "\n",
        "        # Print progress\n",
        "        print(f'Epoch [{epoch + 1}/{num_epochs}], '\n",
        "              f'Train Loss: {avg_train_loss:.4f}, '\n",
        "              f'Train Accuracy: {epoch_train_accuracy:.2f}%, '\n",
        "              f'Val Loss: {avg_val_loss:.4f}, '\n",
        "              f'Val Accuracy: {epoch_val_accuracy:.2f}%')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TUYNUCtEg1ex"
      },
      "outputs": [],
      "source": [
        "import torchvision.models as models\n",
        "import torch.nn as nn\n",
        "\n",
        "# Initialise loss and accuracy lists\n",
        "batch_train_loss = [] # Training loss\n",
        "batch_val_loss = [] # Validation loss\n",
        "batch_train_acc = [] # Training accuracy\n",
        "batch_val_acc = [] # Validation accuracy\n",
        "\n",
        "# Load the model\n",
        "model = models.resnet50(weights=models.ResNet50_Weights.DEFAULT, progress=True)\n",
        "\n",
        "# Number of classes for your specific task (e.g., skin diseases)\n",
        "num_classes = 7  # Adjust based on your dataset\n",
        "model.fc = nn.Linear(model.fc.in_features, num_classes)  # Modify final layer for multi-class classification\n",
        "\n",
        "# Move the model to the appropriate device\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "model = model.to(device)\n",
        "\n",
        "# Define the optimizer and criterion\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
        "criterion = FocalLoss(alpha=1.0, gamma=2.0)  # Use Focal Loss instead of Contrastive Loss\n",
        "\n",
        "# Call the training function\n",
        "train_focal_model(model, train_loader, val_loader, criterion, optimizer, num_epochs=10)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XIpvPVBB7UVT"
      },
      "source": [
        "#### **Evaluation Metrics**\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yEjurxBe9Tjx",
        "outputId": "98cc2735-6e89-4c09-88dd-75181d51e57e"
      },
      "outputs": [],
      "source": [
        "# Load the best performing model based on the validation set\n",
        "model.load_state_dict(torch.load('best_model.pth'))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZCZ-oc0jieSY"
      },
      "outputs": [],
      "source": [
        "from sklearn.metrics import classification_report\n",
        "def evaluate_model(model, test_loader):\n",
        "    model.eval()\n",
        "    all_labels = []\n",
        "    all_preds = []\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for images, labels in test_loader:\n",
        "            images, labels = images.to(device), labels.to(device)\n",
        "            outputs = model(images)\n",
        "            _, predicted = torch.max(outputs.data, 1)\n",
        "            all_labels.extend(labels.cpu().numpy())\n",
        "            all_preds.extend(predicted.cpu().numpy())\n",
        "\n",
        "    print(classification_report(all_labels, all_preds, target_names=val_dataset.labels_df.columns[1:], zero_division=0))\n",
        "\n",
        "evaluate_model(model, test_loader)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mhBf5LeSwlG8"
      },
      "source": [
        "#### **Plot loss and accuracy curves**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 491
        },
        "id": "X4Jl68j-e8-_",
        "outputId": "fee5cebf-ee94-42a8-d966-f7e1d2b15481"
      },
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "train_index = []\n",
        "val_index = []\n",
        "num_epochs = 10 # Adjust number of epochs if needed config 2 - 15, config 1 - 10\n",
        "\n",
        "train_batch = 313\n",
        "val_batch = 8\n",
        "\n",
        "for i in range(1,num_epochs*train_batch + 1):\n",
        "  train_index.append(i)\n",
        "\n",
        "for i in range(1,num_epochs*val_batch+1):\n",
        "  val_index.append(i*(train_batch/val_batch))\n",
        "# Create a figure with subplots\n",
        "plt.figure(figsize=(12, 5))\n",
        "\n",
        "# Plot Loss\n",
        "plt.subplot(1, 2, 1)\n",
        "plt.plot(train_index, batch_train_loss, label='Training Loss', color='blue')\n",
        "plt.title('Loss per batch')\n",
        "plt.xlabel('Batch')\n",
        "plt.ylabel('Loss')\n",
        "plt.legend()\n",
        "plt.grid()\n",
        "\n",
        "# Plot Loss\n",
        "plt.subplot(1, 2, 1)\n",
        "plt.plot(val_index, batch_val_loss, label='Validation Loss', color='orange')\n",
        "plt.title('Loss per batch')\n",
        "plt.xlabel('Batch')\n",
        "plt.ylabel('Loss')\n",
        "plt.legend()\n",
        "plt.grid()\n",
        "\n",
        "# Plot Accuracy\n",
        "plt.subplot(1, 2, 2)\n",
        "plt.plot(train_index, batch_train_acc, label='Training Accuracy', color='green')\n",
        "plt.title('Accuracy per batch')\n",
        "plt.xlabel('Batch')\n",
        "plt.ylabel('Accuracy (%)')\n",
        "plt.legend()\n",
        "plt.grid()\n",
        "\n",
        "# Plot Accuracy\n",
        "plt.subplot(1, 2, 2)\n",
        "plt.plot(val_index, batch_val_acc, label='Validation Accuracy', color='red')\n",
        "plt.title('Accuracy per batch')\n",
        "plt.xlabel('Batch')\n",
        "plt.ylabel('Accuracy (%)')\n",
        "plt.legend()\n",
        "plt.grid()\n",
        "\n",
        "# Show the plots\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NhNjSx0ev4fU"
      },
      "source": [
        "#### **Plot Multiclass ROC curves**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "id": "NnUWpsj6056W"
      },
      "outputs": [],
      "source": [
        "from sklearn.metrics import roc_curve, auc\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "\n",
        "def obtain_predictions(model, test_loader):\n",
        "    model.eval()\n",
        "    all_labels = []\n",
        "    all_preds = []\n",
        "    all_probs = []\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for images, labels in test_loader:\n",
        "            images, labels = images.to(device), labels.to(device)\n",
        "            outputs = model(images)\n",
        "\n",
        "            # Get predicted probabilities using softmax\n",
        "            probabilities = torch.softmax(outputs, dim=1)\n",
        "            all_probs.append(probabilities.cpu().numpy())\n",
        "            all_labels.append(labels.cpu().numpy())\n",
        "\n",
        "            # Get predicted classes\n",
        "            _, predicted = torch.max(outputs.data, 1)\n",
        "            all_preds.append(predicted.cpu().numpy())\n",
        "\n",
        "    all_labels = np.concatenate(all_labels)\n",
        "    all_preds = np.concatenate(all_preds)\n",
        "    all_probs = np.concatenate(all_probs)\n",
        "\n",
        "    return all_labels, all_preds, all_probs\n",
        "\n",
        "# Load the best model and evaluate\n",
        "# model.load_state_dict(torch.load('best_model.pth', map_location=torch.device('cpu')))\n",
        "all_labels, all_preds, all_probs = obtain_predictions(model, test_loader)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DEfYc3g6200f"
      },
      "outputs": [],
      "source": [
        "def plot_multiclass_roc(all_labels, all_probs, num_classes, class_names):\n",
        "    from sklearn.preprocessing import label_binarize\n",
        "    from sklearn.metrics import roc_curve, auc\n",
        "\n",
        "    # Binarize the labels\n",
        "    y_bin = label_binarize(all_labels, classes=range(num_classes))\n",
        "\n",
        "    # Compute ROC curve and ROC area for each class\n",
        "    fpr = dict()\n",
        "    tpr = dict()\n",
        "    roc_auc = dict()\n",
        "\n",
        "    for i in range(num_classes):\n",
        "        fpr[i], tpr[i], _ = roc_curve(y_bin[:, i], all_probs[:, i])\n",
        "        roc_auc[i] = auc(fpr[i], tpr[i])\n",
        "\n",
        "    # Plot ROC curves\n",
        "    plt.figure()\n",
        "    for i in range(num_classes):\n",
        "        plt.plot(fpr[i], tpr[i], label=f'{class_names[i]} (AUC = {roc_auc[i]:.2f})')\n",
        "\n",
        "    plt.plot([0, 1], [0, 1], 'k--')  # Diagonal line\n",
        "    plt.xlim([0.0, 1.0])\n",
        "    plt.ylim([0.0, 1.05])\n",
        "    plt.xlabel('False Positive Rate')\n",
        "    plt.ylabel('True Positive Rate')\n",
        "    plt.title('Multiclass ROC Curve')\n",
        "    plt.legend(loc='lower right')\n",
        "    plt.show()\n",
        "\n",
        "# Number of classes (should match your dataset)\n",
        "num_classes = 7  # Adjust this to your specific number of classes\n",
        "\n",
        "# Plot the multiclass ROC curves\n",
        "class_names = ['MEL', 'NV', 'BCC', 'AKIEC', 'BKL', 'DF', 'VASC']\n",
        "plot_multiclass_roc(all_labels, all_probs, num_classes,class_names)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Nwx27ivYy3Rw"
      },
      "source": [
        "#### **Plot Gradient Class Activation Maps**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "id": "X_iI2_251LzZ"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn.functional as F\n",
        "import torchvision.models as models\n",
        "\n",
        "class GradCAM:\n",
        "    def __init__(self, model, target_layer):\n",
        "        self.model = model\n",
        "        self.target_layer = target_layer\n",
        "        self.gradients = None\n",
        "        self.activations = None\n",
        "\n",
        "        # Register hooks for the target layer\n",
        "        self.target_layer.register_forward_hook(self.save_activation)\n",
        "        self.target_layer.register_backward_hook(self.save_gradient)\n",
        "\n",
        "    def save_activation(self, module, input, output):\n",
        "        self.activations = output\n",
        "\n",
        "    def save_gradient(self, module, grad_input, grad_output):\n",
        "        self.gradients = grad_output[0]\n",
        "\n",
        "    def generate_cam(self, class_idx):\n",
        "        # Get the weights from the gradients\n",
        "        weights = F.adaptive_avg_pool2d(self.gradients, 1)\n",
        "        cam = weights.view(-1) @ self.activations.view(self.activations.size(1), -1)\n",
        "        cam = cam.view(self.activations.size(2), self.activations.size(3))\n",
        "\n",
        "        # Apply ReLU\n",
        "        cam = F.relu(cam)\n",
        "\n",
        "        # Normalize the CAM\n",
        "        cam = cam - cam.min()\n",
        "        cam /= cam.max()\n",
        "\n",
        "        return cam.cpu().data.numpy()\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yj-31s9O07g_"
      },
      "outputs": [],
      "source": [
        "import cv2\n",
        "import matplotlib.pyplot as plt\n",
        "from torchvision import transforms\n",
        "from PIL import Image\n",
        "import numpy as np\n",
        "\n",
        "# Function to preprocess the image\n",
        "def preprocess_image(image_path):\n",
        "    image = Image.open(image_path).convert(\"RGB\")\n",
        "    transform = transforms.Compose([\n",
        "        transforms.Resize((224, 224)),\n",
        "        transforms.ToTensor(),\n",
        "    ])\n",
        "    return transform(image).unsqueeze(0)  # Add batch dimension\n",
        "\n",
        "def plot_gradcam(model, image_paths, target_classes):\n",
        "    model.eval()\n",
        "    grad_cam = GradCAM(model, model.layer4)  # Call GradCAM, you can change the layer to check other layers\n",
        "\n",
        "    # Create a figure for the 6 subplots\n",
        "    plt.figure(figsize=(20, 8))  # Adjust size as needed\n",
        "\n",
        "    for idx, (img_path, target_class) in enumerate(zip(image_paths, target_classes)):\n",
        "        input_tensor = preprocess_image(img_path).to(device)\n",
        "\n",
        "        # Forward pass\n",
        "        output = model(input_tensor)\n",
        "        pred_class = output.argmax(dim=1).item()\n",
        "\n",
        "        # Uncomment to output predicted class\n",
        "        # print(pred_class)\n",
        "        # Backward pass\n",
        "        model.zero_grad()\n",
        "        output[0][target_class].backward()\n",
        "\n",
        "        # Generate Grad-CAM\n",
        "        cam = grad_cam.generate_cam(target_class)\n",
        "\n",
        "        # Load original image for visualization\n",
        "        original_image = cv2.imread(img_path)\n",
        "        original_image = cv2.resize(original_image, (224, 224))  # Resize to match model input\n",
        "\n",
        "        # Convert CAM to heatmap\n",
        "        heatmap = cv2.applyColorMap(np.uint8(255 * cam), cv2.COLORMAP_JET)\n",
        "        heatmap = cv2.cvtColor(heatmap, cv2.COLOR_BGR2RGB)\n",
        "\n",
        "        # Resize heatmap to match original image dimensions\n",
        "        heatmap = cv2.resize(heatmap, (original_image.shape[1], original_image.shape[0]))\n",
        "\n",
        "        # Normalize heatmap\n",
        "        heatmap = heatmap / 255.0\n",
        "\n",
        "        # Overlay heatmap on the original image\n",
        "        superimposed_img = heatmap + original_image / 255.0\n",
        "        superimposed_img /= superimposed_img.max()  # Normalize to [0, 1]\n",
        "\n",
        "        # Define subplot index\n",
        "        subplot_index = idx * 3\n",
        "\n",
        "        # Plot the images\n",
        "        plt.subplot(len(image_paths)//2 + 1, 6, subplot_index + 1)  # Original Image\n",
        "        plt.imshow(original_image)\n",
        "        plt.title('Original Image')\n",
        "        plt.axis('off')\n",
        "\n",
        "        plt.subplot(len(image_paths)//2 + 1, 6, subplot_index + 2)  # Grad-CAM\n",
        "        plt.imshow(cam, cmap='jet', alpha=0.5)\n",
        "        plt.title('Grad-CAM')\n",
        "        plt.axis('off')\n",
        "\n",
        "        plt.subplot(len(image_paths)//2 + 1, 6, subplot_index + 3)  # Overlay\n",
        "        plt.imshow(superimposed_img)\n",
        "        plt.title('Overlay')\n",
        "        plt.axis('off')\n",
        "\n",
        "    plt.tight_layout()  # Adjusts spacing to prevent overlap\n",
        "    plt.show()  # Show all plots together\n",
        "# Example image paths and target classes\n",
        "\n",
        "# class_0 melanoma\n",
        "# image_paths = [\n",
        "#     'ISIC2018_Task3_Test_Input/ISIC_0034595.jpg',\n",
        "#     'ISIC2018_Task3_Test_Input/ISIC_0034650.jpg',\n",
        "#     'ISIC2018_Task3_Test_Input/ISIC_0034657.jpg',\n",
        "#     'ISIC2018_Task3_Test_Input/ISIC_0034687.jpg',\n",
        "#     'ISIC2018_Task3_Test_Input/ISIC_0034737.jpg',\n",
        "#     'ISIC2018_Task3_Test_Input/ISIC_0034764.jpg'\n",
        "# ]\n",
        "\n",
        "# class_1 nv\n",
        "# image_paths = [\n",
        "#     'ISIC2018_Task3_Test_Input/ISIC_0034528.jpg',\n",
        "#     'ISIC2018_Task3_Test_Input/ISIC_0034532.jpg',\n",
        "#     'ISIC2018_Task3_Test_Input/ISIC_0034533.jpg',\n",
        "#     'ISIC2018_Task3_Test_Input/ISIC_0034534.jpg',\n",
        "#     'ISIC2018_Task3_Test_Input/ISIC_0034537.jpg',\n",
        "#     'ISIC2018_Task3_Test_Input/ISIC_0034540.jpg'\n",
        "# ]\n",
        "\n",
        "# class_2 bcc\n",
        "# image_paths = [\n",
        "#     'ISIC2018_Task3_Test_Input/ISIC_0034554.jpg',\n",
        "#     'ISIC2018_Task3_Test_Input/ISIC_0034606.jpg',\n",
        "#     'ISIC2018_Task3_Test_Input/ISIC_0034679.jpg',\n",
        "#     'ISIC2018_Task3_Test_Input/ISIC_0034682.jpg',\n",
        "#     'ISIC2018_Task3_Test_Input/ISIC_0034695.jpg',\n",
        "#     'ISIC2018_Task3_Test_Input/ISIC_0034712.jpg'\n",
        "# ]\n",
        "\n",
        "# class_3 akiec\n",
        "# image_paths = [\n",
        "#     'ISIC2018_Task3_Test_Input/ISIC_0034579.jpg',\n",
        "#     'ISIC2018_Task3_Test_Input/ISIC_0035149.jpg',\n",
        "#     'ISIC2018_Task3_Test_Input/ISIC_0035210.jpg',\n",
        "#     'ISIC2018_Task3_Test_Input/ISIC_0035372.jpg',\n",
        "#     'ISIC2018_Task3_Test_Input/ISIC_0035376.jpg',\n",
        "#     'ISIC2018_Task3_Test_Input/ISIC_0035348.jpg'\n",
        "# ]\n",
        "\n",
        "# class_4 bkl\n",
        "# image_paths = [\n",
        "#     'ISIC2018_Task3_Test_Input/ISIC_0034526.jpg',\n",
        "#     'ISIC2018_Task3_Test_Input/ISIC_0034531.jpg',\n",
        "#     'ISIC2018_Task3_Test_Input/ISIC_0034535.jpg',\n",
        "#     'ISIC2018_Task3_Test_Input/ISIC_0034570.jpg',\n",
        "#     'ISIC2018_Task3_Test_Input/ISIC_0034578.jpg',\n",
        "#     'ISIC2018_Task3_Test_Input/ISIC_0034591.jpg'\n",
        "# ]\n",
        "\n",
        "# class_5 df\n",
        "# image_paths = [\n",
        "#     'ISIC2018_Task3_Test_Input/ISIC_0035239.jpg',\n",
        "#     'ISIC2018_Task3_Test_Input/ISIC_0035552.jpg',\n",
        "#     'ISIC2018_Task3_Test_Input/ISIC_0035821.jpg',\n",
        "#     'ISIC2018_Task3_Test_Input/ISIC_0035844.jpg',\n",
        "#     'ISIC2018_Task3_Test_Input/ISIC_0035904.jpg',\n",
        "#     'ISIC2018_Task3_Test_Input/ISIC_0035940.jpg'\n",
        "# ]\n",
        "\n",
        "# class_6 VASC\n",
        "image_paths = [\n",
        "    'ISIC2018_Task3_Test_Input/ISIC_0034911.jpg',\n",
        "    'ISIC2018_Task3_Test_Input/ISIC_0034739.jpg',\n",
        "    'ISIC2018_Task3_Test_Input/ISIC_0034805.jpg',\n",
        "    'ISIC2018_Task3_Test_Input/ISIC_0034954.jpg',\n",
        "    'ISIC2018_Task3_Test_Input/ISIC_0035194.jpg',\n",
        "    'ISIC2018_Task3_Test_Input/ISIC_0034896.jpg'\n",
        "]\n",
        "\n",
        "# Target classes for each image (index based on your class names)\n",
        "\n",
        "target_classes = [6,6,6,6,6,6] # Adjust based on the class indices (0 - MEL, 1 - NV, 2 - BCC, 3 - AKIEC, 4 - BKL, 5 - DF, 6 - VASC)\n",
        "\n",
        "# Class names for reference\n",
        "class_names = ['MEL', 'NV', 'BCC', 'AKIEC', 'BKL', 'DF', 'VASC']\n",
        "\n",
        "# Plot Grad-CAMs\n",
        "plot_gradcam(model, image_paths, target_classes)"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
